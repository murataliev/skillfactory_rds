{"cells":[{"metadata":{},"cell_type":"markdown","source":"# General information"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/884/1*UDi7KpyFX8gwV1k7aeMS-g.jpeg)\n\nThis was my first experience using a logistic regression for classification purpose, which I have done in the Data Science course from SkillFactory. The legend was that we have a dataset for credit scoring. The task was to build a model for predicting the probability of default of secondary clients. The development time was limited to 48 hours."},{"metadata":{},"cell_type":"markdown","source":"# 1. Initial setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing modules.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom pandas import Series\nfrom sklearn import metrics \nfrom sklearn.feature_selection import f_classif, mutual_info_classif\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.metrics import confusion_matrix, auc, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score \n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Setting the conditions for experiments.\nrandom_seed = 42\ncurrent_date = pd.to_datetime('21/10/2020')\npd.set_option('display.max_columns', None)\ndata_directory = '/kaggle/input/sf-dst-scoring/'\n!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a function for detecting outliers.\ndef outlier_detect(data, column):\n    Q1 = np.percentile(column, 25)\n    Q3 = np.percentile(column, 75)\n    IQR = Q3 - Q1\n    lower_range = Q1 - (1.5 * IQR)\n    upper_range = Q3 + (1.5 * IQR)\n    lower_number = len(data[column<lower_range])\n    upper_number = len(data[column>upper_range])\n    print('Lower Range:', lower_range,\n          'Upper Range:', upper_range,\n          'Lower Outliers:', lower_number,\n          'Upper Outliers:', upper_number, \n          sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a function for visualization of confusion matrix.\ndef show_confusion_matrix(y_true, y_pred):\n    color_text = plt.get_cmap('PuBu')(0.95)\n    class_names = ['Default', 'Non-Default']\n    cm = confusion_matrix(y_true, y_pred)\n    cm[0,0], cm[1,1] = cm[1,1], cm[0,0]\n    df = pd.DataFrame(cm, index=class_names, columns=class_names)\n    \n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), title=\"Confusion Matrix\")\n    ax.title.set_fontsize(15)\n    sns.heatmap(df, square=True, annot=True, fmt=\"d\", linewidths=1, cmap=\"PuBu\")\n    plt.setp(ax.get_yticklabels(), rotation=0, ha=\"right\", rotation_mode=\"anchor\", fontsize=12)\n    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\", rotation_mode=\"anchor\", fontsize=12)\n    ax.set_ylabel('Predicted Values', fontsize=14, color = color_text)\n    ax.set_xlabel('Real Values', fontsize=14, color = color_text)\n    b, t = plt.ylim()\n    plt.ylim(b+0.5, t-0.5)\n    fig.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a function for visualization of metrics for logistic regression.\ndef all_metrics(y_true, y_pred, y_pred_prob):\n    dict_metric = {}\n    P = np.sum(y_true==1)\n    N = np.sum(y_true==0)\n    TP = np.sum((y_true==1)&(y_pred==1))\n    TN = np.sum((y_true==0)&(y_pred==0))\n    FP = np.sum((y_true==0)&(y_pred==1))\n    FN = np.sum((y_true==1)&(y_pred==0))\n    \n    dict_metric['Positive, P'] = [P,'default']\n    dict_metric['Negative, N'] = [N,'non-default']\n    dict_metric['True Positive, TP'] = [TP,'correctly identified default']\n    dict_metric['True Negative, TN'] = [TN,'correctly identified non-default']\n    dict_metric['False Positive, FP'] = [FP,'incorrectly identified default']\n    dict_metric['False Negative, FN'] = [FN,'incorrectly identified non-default']\n    dict_metric['Accuracy'] = [accuracy_score(y_true, y_pred),'Accuracy=(TP+TN)/(P+N)']\n    dict_metric['Precision'] = [precision_score(y_true, y_pred),'Precision = TP/(TP+FP)'] \n    dict_metric['Recall'] = [recall_score(y_true, y_pred),'Recall = TP/P']\n    dict_metric['F1-score'] = [f1_score(y_true, y_pred),'Harmonical mean of Precision Ð¸ Recall']\n    dict_metric['ROC_AUC'] = [roc_auc_score(y_true, y_pred_prob),'ROC AUC Score']    \n\n    temp_df = pd.DataFrame.from_dict(dict_metric, orient='index', columns=['Value', 'Description'])\n    display(temp_df)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing datasets.\ndata_train = pd.read_csv(data_directory+'train.csv')\ndata_test = pd.read_csv(data_directory+'test.csv')\nsample_submission = pd.read_csv(data_directory+'/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the data.\ndata_train.info()\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the data.\ndata_test.info()\ndata_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the datasets.\ndata_train['sample'] = 1\ndata_test['sample'] = 0\ndata = data_train.append(data_test, sort=False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Preliminary data examination & engineering\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the data.\ndata.info()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for missing values.\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the number of unique values.\ndata.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* app_date - date of request, time variable, requires processing.\n* education - level of education, categorical variable, requires processing and missing values correction.\n* sex - binary variable, requires processing.\n* age - continuous variable, requires processing.\n* car - car availability, binary variable, requires processing.\n* car_type - foreign-made car availability, binary variable, requires processing.\n* decline_app_cnt - number of rejected requests, continuous variable.\n* good_work - flag of a well-paid job, binary variable.\n* score_bki - BKI (credit reporting agency) internal score, continuous variable.\n* bki_request_cnt - number of requests to the BKI (credit reporting agency), continuous variable.\n* region_rating - rating of the region, categorical variable.\n* home_address - home address categorizer, categorical variable.\n* work_address - work address categorizer, categorical variable.\n* income - client's income level, continuous variable. \n* sna - level of connection with another clients, categorical variable.\n* first_time - how long the client has been in the database, categorical variable.\n* foreign_passport - passport availability, binary variable, requires processing.\n* default - default in the past, binary target variable.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping column names by data type.\ntime_cols = ['app_date']\ncat_cols = ['education', 'region_rating', 'home_address', 'work_address', 'sna', 'first_time']\nbin_cols = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport']\nnum_cols = ['age','decline_app_cnt','score_bki','bki_request_cnt','income']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the missing values.\ndata['education'].value_counts(dropna = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a new binary variable for missing values in the \"education\"column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new feature.\ndata['education_nan'] = pd.isna(data['education']).astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a temporary measure, let's fill in the missing values with school education."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling the missing values with the most frequent value ('SCH').\ndata['education'] = data['education'].fillna('SCH')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's convert binary variables to numeric format using LabelEncoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding binary variables.\nlabel_encoder = LabelEncoder()\nfor column in bin_cols:\n    data[column] = label_encoder.fit_transform(data[column])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the data.\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Analysis of variables"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Application date (+new feature: timedelta)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the data to datetime.\ndata['app_date'] = pd.to_datetime(data['app_date'], format='%d%b%Y')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what is the earliest request date in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the minimum.\ndata_min = min(data['app_date'])\ndata_min","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's introduce a new variable - the difference between the request date and the minimum."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new feature.\ndata['app_date_timedelta'] = (data['app_date'] - data_min).dt.days.astype('int')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add our new feature to the list of continuous variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a feature to the list.\nnum_cols.append('app_date_timedelta')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['app_date_timedelta'].hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata.boxplot(column=['app_date_timedelta'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detection of outliers.\noutlier_detect(data,data['app_date_timedelta'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Education"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['education'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Everything looks logical: the higher the level of education, the fewer clients apply for a loan. Now let's convert education feature to numeric format using 'map' function."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding a categorical variable.\neducation_dict = {\n    'SCH': 1,\n    'GRD': 2,\n    'UGR': 3,\n    'PGR': 4,\n    'ACD': 5,\n}\n\ndata['education'] = data['education'].map(education_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['sex'].hist(bins=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the bank has much more female clients."},{"metadata":{},"cell_type":"markdown","source":"## 3.4 Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['age'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata.boxplot(column=['age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detection of outliers.\noutlier_detect(data,data['age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People in their mid-30s are more likely to make big purchases, so it seems logical that we have a maximum there. Now let's log the data and look at the distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking the logarithm.\nnp.log(data['age'] + 1).hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution looks normal, so let's keep the logarithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking the logarithm.\ndata['age'] = np.log(data['age'] + 1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata.boxplot(column=['age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detection of outliers.\noutlier_detect(data,data['age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5 Car"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['car'].hist(bins=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the clients in this dataset don't have a car."},{"metadata":{},"cell_type":"markdown","source":"## 3.6 Foreign-made car"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['car_type'].hist(bins=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most car owners from this dataset have a car of domestic production."},{"metadata":{},"cell_type":"markdown","source":"## 3.7 Declined applications"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['decline_app_cnt'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata.boxplot(column=['decline_app_cnt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detection of outliers.\noutlier_detect(data,data['decline_app_cnt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The vast majority of borrowers do not have rejected applications, so our function treats a significant part of the results as outliers. Let's try to correct the situation by taking the logarithm of the column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking the logarithm.\nnp.log(data['decline_app_cnt'] + 1).hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking the logarithm.\ndata['decline_app_cnt'] = np.log(data['decline_app_cnt'] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata.boxplot(column=['decline_app_cnt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detection of outliers.\noutlier_detect(data,data['decline_app_cnt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, the situation has not changed for the better."},{"metadata":{},"cell_type":"markdown","source":"## 3.8 Good work (well-paid job)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['good_work'].hist(bins=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the clients in this dataset don't have a well-paid job."},{"metadata":{},"cell_type":"markdown","source":"## 3.9 BKI score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['score_bki'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata.boxplot(column=['score_bki'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detection of outliers.\noutlier_detect(data,data['score_bki'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution looks normal, and there aren't many outliers."},{"metadata":{},"cell_type":"markdown","source":"## 3.10 BKI requests"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['bki_request_cnt'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata.boxplot(column=['bki_request_cnt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detection of outliers.\noutlier_detect(data,data['bki_request_cnt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking the logarithm.\nnp.log(data['bki_request_cnt'] + 1).hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking the logarithm.\ndata['bki_request_cnt'] = np.log(data['bki_request_cnt'] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata.boxplot(column=['bki_request_cnt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detection of outliers.\noutlier_detect(data,data['bki_request_cnt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logarithmization significantly improved the situation, the number of outliers was reduced from 2636 to 15.\n"},{"metadata":{},"cell_type":"markdown","source":"## 3.11 Region rating"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['region_rating'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.12 Home address"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['home_address'].hist(bins=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.13 Work address"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['work_address'].hist(bins=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.14 Income"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['income'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata.boxplot(column=['income'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\noutlier_detect(data,data['income'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking the logarithm.\nnp.log(data['income'] + 1).hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking the logarithm.\ndata['income'] = np.log(data['income'] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata.boxplot(column=['income'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detection of outliers.\noutlier_detect(data,data['income'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logarithmization improved the situation, the distribution changed from lognormal to normal, and the number of outliers decreased from 7000 to 2609."},{"metadata":{},"cell_type":"markdown","source":"## 3.15 SNA (level of connection with other clients)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['sna'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.16 First time (how long the client has been in the database)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['first_time'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.17 Foreign passport"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['foreign_passport'].hist(bins=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the clients in this dataset don't have a foreign passport."},{"metadata":{},"cell_type":"markdown","source":"## 3.18 Default"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\ndata['default'].hist(bins=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The balance of classes in our dataset is strongly skewed towards customers who do not have a default."},{"metadata":{},"cell_type":"markdown","source":"# 4. Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the correlation matrix\ndata_train_temp = data[data['sample']==1]\nsns.heatmap(data_train_temp[num_cols].corr().abs(), vmin=0, vmax=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the correlation matrix\ndata_train_temp[num_cols].corr().abs().sort_values(by='decline_app_cnt', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There were no strong correlations between the features, except for the number of declined applications and the BKI score. Unfortunately, I didn't have enough time to test the model with a different set of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the frequency distribution.\nfig, axes = plt.subplots(2, 3, figsize=(15, 15))\naxes = axes.flatten()\nfor i in range(len(num_cols)):\n    sns.boxplot(x=\"default\", y=num_cols[i], data=data_train_temp, ax=axes[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Default clients are on average younger and earn less, have more applications and rejections from the bank, but simultaneously they have a higher BKI rating."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the importance of features.\nimp_num = Series(f_classif(data_train_temp[num_cols], \n                           data_train_temp['default'])[0], index = num_cols)\nimp_num.sort_values(inplace = True)\nimp_num.plot(kind = 'barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the importance of features.\nimp_cat = Series(mutual_info_classif(\n    data_train_temp[bin_cols + cat_cols], data_train_temp['default'], \n    discrete_features =True\n), index = bin_cols + cat_cols)\n\nimp_cat.sort_values(inplace = True)\nimp_cat.plot(kind = 'barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of the continuous variables, the most important are the number of declined applications and the BKI rating. Of the categorical and binary non-variables, the most important are connections with other clients and the time spent in the database."},{"metadata":{},"cell_type":"markdown","source":"# 5. Data preprocessingÂ¶\n"},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardization of data.\nss = StandardScaler()\ndata[num_cols] = pd.DataFrame(ss.fit_transform(data[num_cols]),columns = data[num_cols].columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the data.\ndata.info()\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Using RandomForestRegressor to fill missing values in \"education\" feature"},{"metadata":{},"cell_type":"markdown","source":"Let's use the knowledge gained in [previous competition](https://www.kaggle.com/ogurrw/sf-tripadvisor-rating-akbar-murataliev) to predict the missing values in the \"education\"column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data processing and model training.\ndata_temp = data.drop(['sample', 'client_id', 'app_date', 'default'], axis=1)\ndata_education_nan = data_temp[data_temp['education_nan']==1]\ndata_no_nan = data_temp[data_temp['education_nan']==0]\ny = data_no_nan['education'].values\nX = data_no_nan.drop(['education'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = random_seed)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state = random_seed)\nmodel.fit(X_train, y_train)\ny_pred = np.round(model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the values.\npredict = np.round(model.predict(data_education_nan.drop(['education'], axis=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding predicted values to the dataset.\nindex_education_nan = data[data['education_nan']==1].index\ndata.loc[index_education_nan,'education'] = predict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 One-hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding categorical variables.\ndata = pd.get_dummies(data, prefix=cat_cols, columns=cat_cols)\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the data.\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset.\ndata_train = data.query('sample == 1').drop(['sample', 'client_id', 'app_date'], axis=1)\ndata_test = data.query('sample == 0').drop(['sample', 'client_id', 'app_date'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training and predicting.\nX = data_train.drop(['default'], axis=1)\ny = data_train['default'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_seed)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\ny_pred_prob = model.predict_proba(X_test)[:,1]\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC curve\nprobs = model.predict_proba(X_test)\nprobs = probs[:,1]\n\nfpr, tpr, threshold = roc_curve(y_test, probs)\nroc_auc = roc_auc_score(y_test, probs)\n\nplt.figure()\nplt.plot([0, 1], label='Baseline', linestyle='--')\nplt.plot(fpr, tpr, label = 'Regression')\nplt.title('Logistic Regression ROC AUC = %0.3f' % roc_auc)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the confusion matrix.\nshow_confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the metrics.\nall_metrics(y_test, y_pred, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ROC AUC score is good, but the confusion matrix shows us that our model predicts defaults very poorly. Of the 1827 defaults, only 40 were correctly predicted, or about 2 percent (very low Recall). Let's try to improve the situation by using custom hyperparameters."},{"metadata":{},"cell_type":"markdown","source":"# 8. Regularization"},{"metadata":{},"cell_type":"markdown","source":"The code for regularization was taken from [here](https://www.kaggle.com/sokolovaleks/sf-dst-10-creditscoring-golobokov-sokolov) and slightly modified. Kudos to Alexandr Sokolov and Andrey Golobokov."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(random_state=random_seed)\n\niter_ = 50\nepsilon_stop = 1e-3\n\nparam_grid = [\n    {'penalty': ['l1'], \n     'solver': ['liblinear', 'lbfgs'], \n     'class_weight':['none', 'balanced'], \n     'multi_class': ['auto','ovr'], \n     'max_iter':[iter_],\n     'tol':[epsilon_stop]},\n    {'penalty': ['l2'], \n     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n     'class_weight':['none', 'balanced'], \n     'multi_class': ['auto','ovr'], \n     'max_iter':[iter_],\n     'tol':[epsilon_stop]},\n    {'penalty': ['none'], \n     'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], \n     'class_weight':['none', 'balanced'], \n     'multi_class': ['auto','ovr'], \n     'max_iter':[iter_],\n     'tol':[epsilon_stop]},\n]\n\ngridsearch = GridSearchCV(model, param_grid, scoring='f1', n_jobs=-1, cv=5)\ngridsearch.fit(X_train, y_train)\nmodel = gridsearch.best_estimator_\n\nbest_parameters = model.get_params()\nfor param_name in sorted(best_parameters.keys()):\n        print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n\npreds = model.predict(X_test)\nprint('Accuracy: %.4f' % accuracy_score(y_test, preds))\nprint('Precision: %.4f' % precision_score(y_test, preds))\nprint('Recall: %.4f' % recall_score(y_test, preds))\nprint('F1: %.4f' % f1_score(y_test, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have identified optimal hyperparameters. Now let's train the model using these hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training and predicting.\nmodel = LogisticRegression(random_state=random_seed, \n                           C=1, \n                           class_weight='balanced', \n                           dual=False, \n                           fit_intercept=True, \n                           intercept_scaling=1, \n                           l1_ratio=None, \n                           multi_class='auto', \n                           n_jobs=None, \n                           penalty='l1', \n                           solver='liblinear', \n                           verbose=0, \n                           warm_start=False)\n\nmodel.fit(X_train, y_train)\n\ny_pred_prob = model.predict_proba(X_test)[:,1]\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC curve\nprobs = model.predict_proba(X_test)\nprobs = probs[:,1]\n\nfpr, tpr, threshold = roc_curve(y_test, probs)\nroc_auc = roc_auc_score(y_test, probs)\n\nplt.figure()\nplt.plot([0, 1], label='Baseline', linestyle='--')\nplt.plot(fpr, tpr, label = 'Regression')\nplt.title('Logistic Regression ROC AUC = %0.3f' % roc_auc)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the confusion matrix.\nshow_confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the metrics.\nall_metrics(y_test, y_pred, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks to the use of hyperparameters, Recall increased from 2 to 68 percent. Unfortunately, the Precision has decreased, because now our algorithm is more likely to detect defaults even where there are none. Harmonic mean between Recall and Precision (F1 Score) also decreased. However, from the point of view of the consumer, i.e. the bank, we can say that the quality of the model has increased. However, there is still enough room for improvement."},{"metadata":{},"cell_type":"markdown","source":"# 9. Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = data.query('sample == 1').drop(['sample', 'client_id', 'app_date'], axis=1)\ndata_test = data.query('sample == 0').drop(['sample', 'client_id', 'app_date'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data_train.drop(['default'], axis=1)\ny_train = data_train['default'].values\nX_test = data_test.drop(['default'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(random_state=random_seed, \n                           C=1, \n                           class_weight='balanced', \n                           dual=False, \n                           fit_intercept=True, \n                           intercept_scaling=1, \n                           l1_ratio=None, \n                           multi_class='auto', \n                           n_jobs=None, \n                           penalty='l1', \n                           solver='liblinear', \n                           verbose=0, \n                           warm_start=False,\n                           max_iter=1000)\n\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_prob = model.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.DataFrame(data.query('sample == 0')['client_id'])\nsubmit['default'] = y_pred_prob\nsubmit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10. Recap & Conclusions"},{"metadata":{},"cell_type":"markdown","source":"Let's follow the actions taken:\n\n* We initialized necessary libraries, set visualization conditions and loaded the dataset.\n* We analyzed the features, identified the target variable, looked at external sources, and suggested which features we can rely on for feature engineering.\n* We checked each variable, frequency distributions and created several new features.\n* We filled in the missing values by training the model on the available data.\n* We encoded categorical variables and standardized the data.\n* We trained logistic regression on the available data and evaluated the quality of its prediction using confusion matrix, ROC AUC, and other metrics.\n* We selected hyperparameters and trained the model on them, improving its consumer qualities.\n\nThe following conclusions can be drawn from the results:\n* We can fill in the missing values using a prediction model based on the available data.\n* Built-in sklearn features make pre-processing of data much easier.\n* We should not blindly focus on metrics, the consumer qualities of the model are also very important."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
